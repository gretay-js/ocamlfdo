# Design of OCamlFDO

### Notes on the workflow

Function reordering only requires the second build to repeat the final
linking step using a linker script generated by the ocamlfdo tool. In
particular, function reordering does not use the decoded profile.

Block reordering requires rebuilding from scratch the entire code base
that the executable depends on, including all the libraries and
dependencies. If a dependency is not rebuilt for whatever reason, it
can still be used by the the build, but will not be optimized for
basic block layout.

The linker script + decoded profile pair should be much smaller in
size than the original perf output files or the executables.

###  Correctness

Compared to executables built with ocamlopt in the normal manner, the
only changes with FDO are in terms of code layout. At runtime, this
can affect execution time and any functionality depended on it, such
as performance counters observed by the program. In particular, it
does not affect type safety and provides the same correctness
guarantees as the standard compilation.

OCamlFDO works by transforming OCaml code from a linearized
representation within the compiler (not much higher-level than
assembly language) to a different representation where the in-memory
order of basic blocks is described independently from the semantics of
the program. The terms are then transformed back to the original
linearized representation. The remainder of the pipeline remains the
same as the normal OCaml compiler.

This means that the correctness property for FDO build reduces down to
showing that the two transformations between the linearized code and
the new representation preserve the semantics. We have actually shown
a stronger property, for the whole of jane at some particular
revision: going from the linearized code to the new representation and
back again---when no block reordering is in effect---is the identity
on the terms of the intermediate language. We aim to confirm this
property by careful code review of what is a very modest amount of
code.

We have also performed test runs of randomized ordering of basic blocks.

### Compile-time performance
We don't have numbers for this yet.

The ocamlfdo decoding step is slow and expensive in terms of memory,
because it depends on processing the entire binary and the perf
data. Our binaries can be as big as 1GB. Perf data files can also grow
very quickly, depending on sampling rates---they can easily be larger
than the binary being profiled. It is not uncommon to see 30GB per
perf data file. The decoded profile produced by ocamlfdo is usually
much smaller.

A fresh build using ocamlfdo is not much slower than a build using
ocamlopt, but will likely require more memory (not measured exactly
yet but unlikely to be more than twice as much). The transformation
between the existing and new intermediate languages used in ocamlfdo
just perform a single traversal. However, the heuristics to compute
layout optimizations can make several passes, although their
asymptotic complexity is linear.

The second build step reuses most of the artifacts of the first build
step except for the last compiler phase of transforming linearized
code into assembly. This saves a lot of build time because we don't
need to do any type checking, Flambda transformations, register
allocation, etc.

Format of decoded profile
This is the main remaining design question. The current proposal is to store a decoded profile as a single file in hg. This is similar to the way other FDO tools (1,3) at Facebook and Google save execution profiles under source control. This will be particularly helpful when we add the ability to reuse the profiles after code changes. For now, it means that builds on different machines including on hydra can reuse decoded profiles.

As an upper estimate of the size of profiles, the information stored in the profile refers to a fraction of the text segment of the executable, and the information is amenable to compaction. If the profiles turn out to be too big for hg, we will consider alternative storage in /j/office, probably similarly to the recommend storage of large test inputs.

Fast access and search through decoded profile is important to keep compilation times low, when a profile for a particular function needs to be fetched. The decoded profile will be used by multiple processes of jenga during a parallel build. Having the profile all in one file might become a bottleneck for FDO builds.

We discussed some ways to cope with it. Generating mutliple files (one per function or compilation unit) will be too expensive and harder to express in jenga rules due to unkown number and names of targets.

A better option would be to choose an indexed format that can be queried quickly from a file. Another possibility is a data-base or a service for profile information (see for example (3)).


### Seeing what has happened
The ocamlfdo tool can generate a report, similar to the Flambda inlining reports, describing its reordering decisions.

### Reuse of profiles in the face of source code changes

This is not yet supported. For now, we require that exactly the same
code is used in the two build steps of the executable. Compilation
fails the code a profile applied to is not the same as the code that
was used to collect the profile. We would like to relax this
requirement in the future.

Ideally, we should be able to reuse some of the preprocessed profile
data even after changes have been made to some subset of the source
code of the executable. This means we need to check that the profiling
info remains relevant in some way; when it is not (for example because
the corresponding source code has changed), it must be invalidated. It
is not yet clear how to implement such a system. If a function
changes, its profile should be invalidated, but what about profiles of
its callees, for example?

We have designed a simple check (not yet implemented but should be
forthcoming soon). We can detect at the level of individual functions
or compilation units that the internal representation hasn't changed,
before applying any feedback-directed optimizations. Currently, the
check is quite expensive and involves emitting an additional symbol
per function or compilation unit in the generated assembly to record a
digest of the corresponding code. Default is per function, but it can
be changed with a flag passed to ocamlfdo.

We would like to get user feedback on what action should be taken if
the check fails, based on what scenarios invalidate a profile in a
typical workflow with ocamlfdo.

For example, if the profile is not valid, we could continue
compilation without it. This event should be logged but there is a
risk that stale profiles will not be noticed, causing silent
performance degradation.

Interaction with Jenga's shared cache
The Jenga cache will work but it won't reuse artifacts from non-FDO builds. Note that there is also less sharing in BUILD_PROFILE=fast-exe, which is likely to be on for optimized builds.

Optimizing C stubs
Perf data that we collect covers C stubs in our code base and C code in the OCaml runtime (mainly, the garbage collector).

Our modifications to the OCaml compiler cause the runtime and standard libraries to be compiled in such a way that each C function is in its own section. This means that reordering of such functions will work just like reordering of OCaml functions. All C code built by Jenga will also be compiled in this manner when using FDO.

For block reordering, the situation is more complicated. We do not yet have a proposal as to how to apply block reordering to runtime functions, except by adjusting the source code to force specific orderings. The difficulty is that we will need a separate build of the runtime and standard library for each profile. It seems likely that profiles for the majority of programs will imply a single particular ordering of blocks for these functions.

It may be useful to apply FDO to the code in the standard libraries. We plan to measure the performance impact of this first. If it proves significant, we will probably need to change the build process to handle standard library separately. The work on cross-compilation may already have similar changes planned.

For C functions outside of the runtime, we could use an external open source tool by Google, which is based on (3). The tool transforms perf output files to the formats expected by GCC and LLVM. These compilers could then be invoked in a similar manner to ocamlfdo to recompile based on the observed profiles.

Re-profiling an already-optimized executable ("iterative FDO")
It is possible to re-profile an optimized executable---however, this will require an option to be specified to the ocamlfdo tool via OCAMLFDOFLAGS environment variable. This option will cause the optimized executable to retain the DWARF information needed for FDO rather than setting it back to the information required for typical debugging. (We hope in the future to be able to support both kinds of information within a single executable, but this might require changes to gdb.)

There are downsides to iterative FDO. First, it can result in unstable performance, as shown in the AutoFDO paper (3). Secondly, the executables will be even larger than usual, because the DWARF line number programs embedded within them are larger for executables that can be FDO profiled. (The reason is that sufficient information is stored to map every machine instruction back to the corresponding instruction in the linearized intermediate representation used by backend of the compiler.)

DWARF information is stored in non-mapped sections of the binary and thus should not have any impact on runtime performance. No binary produced during the FDO process, whether equipped with information so it can be FDO-profiled or not, should exhibit any change in semantics from a binary produced using our normal methods.

Is there a plan to upstream FDO into the official OCaml compiler?
In an ideal world we would aim for the following situation:

Patches merged upstream in OCaml to provide some infrastructure to allow the compiler to save the linearized intermediate representation to a file; and to start compilation again from such a file, not an OCaml source file.

Patches merged upstream in OCaml to provide the "technical" functionality we need, such as support for functions in the own sections, and DWARF line number programs with very fine-grained location information.

ocamlfdo released as a standalone tool, which could be used in conjunction with a compiler having support as in points 1 and 2.

Dune rules so that users can perform FDO builds easily.

We may also consider trying to upstream the new intermediate representation as it could be a good target for other optimisations. This part of the code has deliberately been written using only the compiler's standard library to allow for this case.

Discussions with upstream developers will decide whether the above situation is reached or not. It seems important to enable people to try FDO easily outside of the firm so that its benefits can be demonstrated prior to an upstreaming push.

What other tools exist for doing this?
AutoFDO
AutoFDO (3) is an open source tool developed at Google. AutoFDO maps perf profile information back to source level, and then propagates it through the compilation pipeline to guide optimization decisions such as inlining. It has been used for C/C++ code at Google for several years.

The approach we take with ocamlfdo is similar to AutoFDO in that we use profile information to guide optimization decisions performed by the compiler. This approach makes it easier to reason about correctness of generated code.

The difference is that ocamlfdo directly maps sampled data to where it is most relevant in the compiler pipeline. This improves precision of execution profile.

BOLT
BOLT (1) is an open source tool developed at Facebook. It works by directly rewriting an ELF executable file. It takes into account profile information to make optimization decisions, mainly about code layout, but operates at the level of machine instructions rather than any higher-level representation. BOLT has been thoroughly tested and used for several years at Facebook on C and C++ code produced by different C/C++ compilers. It is written in C++ and relies on the LLVM compiler framework.

We used BOLT to estimate the potential performance improvement from code layout optimizations on our code. We found significant potential, which lead to the developement of ocamlfdo. The design of ocamlfdo is influenced by BOLT and we rely on BOLT for testing.

BOLT does not require the binary to be recompiled and doesn't even require any source code. BOLT does however require the binaries to be built with extra information. This is so that it can identify which addresses featuring in the machine code were computed by the linker as a result of applying a relocation. Such calculations may have to be redone if the layout changes. Unfortunately preserving linker relocations causes a significant increase in the size of binaries (more than 50% in our benchmarks). ocamlfdo does not rely on linker relocations, instead relying on different DWARF debugging information, which is much smaller.

BOLT implements code layout heuristics that are not yet implemented in ocamlfdo. These heuristics have been tested and proved beneficial on many C/C++ programs at Facebook. We plan to evaluate these and other heuristics on OCaml code.

BOLT implements transformations that are not yet available in ocamlfdo, such as peephole optimizations, basic block alignment, and hardware specific macro-op fusion.

BOLT optimizes the entire binary including all statically-linked libraries, no matter what source language they come from and whether they can be rebuilt from source or not. ocamlfdo only works on OCaml (.ml) code.

BOLT has a view of the whole program that enables optimizations currently not implemented in the OCaml compiler. Our experience with BOLT also indicates the potential of link-time optimizations for OCaml compiler such as dead code elimination and identical code folding.

ocamlfdo may gain the capability of using profile information to guide inlining decisions and other Flambda optimizations in the future. This is likely a powerful feature and one whose performance benefits may outweigh some of the lower-level whole-program optimizations performed by BOLT.

BOLT did not apply "out of the box" to OCaml native code binaries even though such binaries are standard ELF. We modified BOLT to correctly handle some aspects related to OCaml garbage collection (frame tables and code pages). The changes will be upstreamed as soon as we get clearance from legal. We expect that external users of OCaml will be eager to try BOLT for OCaml. We hope that they might help with maintainance of it too, especially OCaml fans at Facebook.

We are farely confident that OCaml executables are now handled correctly by BOLT and it forms a useful tool for analysing the performance of executables. However at least for the moment we would like to avoid running production executables that have been processed by BOLT. It may be wise to install some kind of watermarking scheme to try to check this automatically. The code of BOLT would be lengthy and problematic to review. It seems likely that the majority (if not all) of the benefit can instead be obtained through ocamlfdo with much higher correctness guarantees.

It is worth noting that at Facebook, BOLT delivers performance improvement on top of the combination of link-time optimization and AutoFDO.

References
(1) BOLT: A Practical Binary Optimizer for Data Centers and Beyond. Maksim Panchenko, Rafael Auler, Bill Nell, and Guilherme Ottoni. In "Proceedings of 2019 International Symposium on Code Generation and Optimization (CGO 2019)".

(2) Optimizing function placement for large-scale data-center applications. Guilherme Ottoni and Bertrand Maher. In "Proceedings of the 2017 International Symposium on Code Generation and Optimization (CGO 2017)".

(3) AutoFDO: automatic feedback-directed optimization for warehouse-scale applications. Dehao Chen, David Xinliang Li, and Tipp Moseley. 2016. In "Proceedings of the 2016 International Symposium on Code Generation and Optimization (CGO 2016)".

(4) Taming Hardware Event Samples for Precise and Versatile Feedback Directed Optimizations. Dehao Chen, Neil Vachharajani, Robert Hundt, Xinliang D. Li, Stéphane Eranian, Wenguang Chen, Weimin Zheng. In "IEEE Transactions on Computers 2013"



* Structure of ocamlfdo

Internally, ocamlfdo is implemented as a compiler driver, statically linked
against the compiler libraries.

OcamlFDO operates at the level of compiler's intermediate
representation (IR) called Linear.  Linear IR is generated by the
compiler backend at the very end, right before emitting assembly.

During decode, OcamlFDO maps the binary addresses that appear in perf
data of the original binary to the corresponding elements of the
Linear IR.  This is achieved using the extra dwarf information
contained in the original binary.  Aggregated and decoded samples from
perf data are saved as a decoded profile.

When a function is compiled, OcamlFDO takes Linear IR and constructs a
control flow graph (CFG) of the function. It can perform some
optimizations on the CFG, such as dead code elimination and
simplification of condition branches.  Then, ocamlfdo converts the
decoded profile from Linear IR to CFG.  This information is used by
heuristics that compute new block layout and finally apply it when
converting CFG back to Linear IR.

This design makes the decode and optimization independent of the
source code of the program. To take advantage of this, we split the
compilation pipeline and generate intermediate representation files
containing Linear IR.  The build system needs to treat "linear IR"
explicitly in its rules.

* Jenga rules for ocamlfdo

Jenga rules have some constraints that we need to work with when
designing the integration:
- target files must be generated in the same directory as the source
  (or under the same tree?).
- we cannot have two targets with the same name in the same
  invocation of jenga.

Jenga rules for FDO builds are not the same as a normal build, because
FDO build can reuse most of the artifacts of the previous build, as
mentioned above. To achieve it, we split compilation rule for .ml
files into two phases: main compilation (that includes all the
frontend, middle-end and backend) and emit assembly.  The first phase
generates a file called .linear per compilation unit. The second phase
takes as input .linear and emits assembly or object file.
We call ocamlfdo between these two phases to transform .linear file,
using the decode profile, if exists.

In the initial build, no CFG is constructed and we only add extra
debug information to linear IR. Therefore, the initial build should
not be much slower than normal compilation because there is no
profile.  Then, in the FDO build, only the second phase of compilation
needs to be repeated.

Contents of .linear file
- magic number for linear
- Linear IR
- Cmm constants
- State from Proc

Decoded profile is a single file, indexed by compilation unit for fast
access.  From this point on, the profiling information can be handled
on a per-compilation-unit basis.

Jenga checks if the profile is present.
If the profile is present, jenga invokes phase 2 with .linear as
input and provides the known location of the profile file,
using WITH_FDO environment variable.
ocamlfdo writes ".fdo.linear" file which is the same as the incoming ".linear"
file except that the code inside functions may have been rearranged at
the level of basic blocks (but not between functions, and functions
are not reordered here). Other optimizations can also be done such as
dead code elimination.

Linker rule in jenga is modified to generate linker script from template
and hot functions layout, as discussed above.

The decoded profile and the generated linker script live in the same
directory as the target executable.

These two extra files under source control become invalid when the
source code changes until perf and decode are rerun.  Also, they will
increase the disk space needed for the project (especially the decoded
profile which can be GBs big).

When WITH\_FDO is set, jenga uses different compilation rules,
for split compilation, and additional intermediate files
are generated. This allows us to speed up the second (fdo) build.

Potentially, any part of the tree might have to be recompiled using
profile information to create an optimized version of the
executable. To identify the location of the profile, WITH\_FDO
environment variable is used (and not jbuild file for the
executable).

* Validate profiles

We also need a way to check that they are valid. Can we store a hashes
of the linear IR into the executable produced in build1 (*). Decode
can copy that information into fdo.profile file.

Then, during phase 2 with a profile, we can check that the hash from
profile we are about to apply for a given compilation unit matches the
md5 of the unit's linear IR that we are about to optimize.  If the .ml
changed and compilation complication produced a different .linear file
(i.e., ), then its hash won't match the decoded profile information
when we try to apply FDO to it.

** How to store hash of linear IR into executable:
A clean way of doing it is through debug info but the patch that does
it was rejected upstream and requires rework.

Temporary solution: for each function (or compilation unit), emit a
symbol in the assembly that includes in its name the name of the unit
and the hash. Use a naming scheme that distinguish it from normal
symbols generated by ocaml compiler/
Then, decode the binary using Owee  and find all symbols that match
this pattern and parse them into mapping of unit / function names to
hashes. This is basically what the debug info does but in a more
principled way.

** PPX rebuild problem

There are PPX executables that get rebuilt by jenga and then used to
build other targets.
It is possible that ppx executable uses a library that is also used by
the target executable that we are optimizing using FDO.
If we have any profile information for that shared library, it will
trigger rebuild of the library and then jenga will rebuild everything
that depends on it, which means it will rebuild the ppx executable,
and then will rebuild all the files that use this ppx.
We can't restrict the libraries uses by PPX because PPX can be
contributed to open source from outside Jane and then used in Jane.

We need to check if there is any sharing in our benchmarks and we
don't expect there to be because of inlining of libraries, benchmarked
are already optimized and use libraries sparingly ppx uses their own
copy of base.

Soluton 1: use dune.
Dune splits compilation into two steps: first run ppx on .ml and
generate parse tree. The result is saved in a file. Then, call ocaml
compiler with the marshaled parse tree as input instead of the
original source file. The compiler will skips all the parsing.
In our context, it would prevents some of the redundant build, but not
all of it. PPX executable will be rebuilt and rerun on its input .ml
files, but the resulting marshaled parse tree files will be identical
to what was generated in build1. Jenga will detect that and will not
call the compiler on it, but proceed directly to calling fdo (step 5
above).

Solution 2: use cross compilation (available with dune already or soon):
We treat ppx executables generated in build1 as "host tools".  In
build2, we perform cross compilation: we use "host" tools (i.e., ppx
executables produced in step 1) to generate targets in step 2.
Therefore, we never need to rebuild .ml files that haven't changed
becuase ppx executables won't change, even if there are shared
libraries. This will also solve the problem of other code generation
tools.

* C stubs rule

To use profile-guided optimization with C stubs, the user has to
manually generate autofdo profile for gcc.  Then, Jenga will check for
existance of this profile file in the same way it looks the decoded
profile (using the location of the executable refered to by WITH_FDO).
Jenga will add a -fauto-profile=<filename> flag to the compilation
rules of C files only.

For example, if WITH_FDO=pal.exe then generate profile for gcc using
#+BEGIN_SRC bash
$ create_gcov --profile=perf.data --gcov=pal.gcov --binary=pal.exe -gcov_version=1
#+END_SRC
and Jenga will add "-fauto-profile=pal.gcov" to OCAMLCFLAGS.

* Linker script

Linker scripts depend on the version of the system linker LD, the
version and the configuration of the OS where the final build run.

We can obtain a default linker-script file for example using GNU
linker "ld --verbose", but it needs to be changed slightly (delete
comments) before it can be passed back to ld. We also need to identify
the correct place in the linker-script to import hot functions layout.
Therefore, we will create template linker scripts for each configuration
we support and place them in a fixed location (like the compiler's location).

We will use a special rule in jenga for creating the linker script
when WITH_FDO is set.  The rule will look up the correct template and
combine it with the hot function layout, generated earlier by Decode,
before passing it to ld.

This not yet implemented but should not be a problem with a special
jenga rule.

** Manual experiments with function reordering

Experiments can be made by manually reordering functions in the hot
functions layout, or adding functions that are known to be called
often in conjunction with functions in the script. Jenga will detect
change and regenerate linker script using updated layout, and
then will repeat link of the target executable.

Jenga will overwrite hot functions layout file when a change of the
decoded profile file is detected and triggers a build.

The generated linker script can be added to hg, after renaming it so
as not to clash with names of jenga targets.  This is no recommended
because function names include identifiers generated by the compiler
that can (and will) change between builds. The script can be
generalized by removing names of "anon" and replacing function
identifiers with "*" but it is not robust enough against changes in
compiler and source.

The combined linker script can also be used in a non-FDO build by
adding the following line to jbuild and building using a compiler with
function-sections enabled:
#+BEGIN_SRC
(ocamlopt_flags (:standard -ccopt "-Xlinker --script=linker-script"))
#+END_SRC
BUT THIS IS NOT RECOMMENDED. See above!


* How to build ocamlfdo?

Currenly, ocamlfdo is build using dune and opam, outside of jenga.

We need to build ocamlfdo using the default compiler, to match the
version of compiler libs.  We need ocamlfdo binary to be available on
the path to jenga. One way to do both is to put ocamlfdo inside jane,
and build it using jenga. Another way is to install it in a standard
place, for example in the compiler install directory right next to
ocamlopt. The latter will make it easier for ocamlfdo to replace
ocamlopt, but would require changes to internal releases. The former
would work better if the compiler needs to be rebuild with profile
info.

We assume for now that standard libraries and compiler do not need to
be rebuilt with profiling information in order to get the most
performance benefit from FDO.  If that turns out not to be the case, a
different integration will be needed.  We considered rebuilding the
compiler in a tmp directory in the workspace, and changing the path to
ocamlopt for jenga to trigger new builds.


* Compiler toolchain requirements

The same compiler must be used for building ocamlfdo itself and jenga
builds (because the optimizations rely on the low-level internal
representation).

The compiler needs to be invoked with -function-sections for
function reordering using linker script to work.  With this option, the
compiler places each function in a separate section. The order and
alignment of function sections can be decided at link time.

* Testing and benchmarking strategy

** Correctness of CFG construction
Property: transformation Linear->CFG->Linear is identity.

We applied the following test during build of all of the code in Jane:
Given a function's Linear IR, translate it to CFG, and then back to
linear IR, and check that the result is identical to the input.

Interestingly, the only tests that failed were because CFG
construction naturally eliminated some dead code that was hard to
detect at LinearIR.  This lead to a couple of patches to improve
ocamlopt's dead code elimination. It also indicates that CFG is a good
IR to have in the compiler. Hopefully this will help with upstreaming.

** Correctness of reordering of basic blocks

Property: given a CFG and an arbitrary layout, CFG->Linear produces
correct code.

We check it use random layout and running all the tests in Jane:
given a CFG and its original layout, randomly reorder the blocks.

** Correctness of profile aggregation and decoding
Property: execution counters are correctly aggregated and mapped to
correct functions and blocks.

On our benchmarks, we compare the decoded profile computed by ocamlfdo
to that of BOLT. This can be done semi-automatically by (a) ocamlfdo
profile in BOLT format, or (b) parisng in BOLT format and comparing.

So far, we have only performed (a) on a small test. There is complete
match at the level of function counters. The block-level counters are
not matching completey, because there are differences in the
interpretation of sizes of instructions.  Check (b) will be used to
address it. Another difference is that ocamlfdo does not handle data
on PLT function yet.

BOLT can generate linker scripts that can be used directly with the OCaml
compiler when linking, without needing ocamlfdo itself, if all that is wanted
is function reordering.

We have also modified BOLT to generate basic block layouts. ocamlfdo parses
these layouts, which refer to raw binary addresses, and maps them to its
intermediate representation. ocamlfdo can save the "relative" layout to a file
for later use. ocamlfdo can read relative layout files and perform block
reorderings based on them.

** Benchmarks

- pal testbed
- books medusa benchmark tse-flex and nse-currency
- start/srt







* Service for profiles

Given a function name and the unique identifier of the build artifact,
return the corresponding decoded perf profile, if there is one that is
valid for the artifact.

Whenever a new decoded profile is ready, it would need to be picked up
by the service automatically. The services needs to know when to
invalidate stale profiles kept in the service.

If we have build-id per executable, the service can merge profiles
collected from different executions that match build-id. If build-id
is at the level of function or compilation unit, we will need to
invalidate them if any other part of the app has changed.

* Action items

- code review ocamlfdo
- implement patch to jenga support for ocamlfdo
- implement compiler changes for -stop-after and -start-from
- upstream compiler patches
- upstream bolt changes for ocaml
- add watermark to bolted binaries internally
- enable bolt internally
- upstream owee changes
- implement a check that code and profile match
- setup installations of ocamlfdo in /j/office/ alongside the default compiler
- ensure that compiler toolchain and ocamlfdo match
- email tools-compilers-ambassdors to collect more early benchmarks
- encourage users to try out an early version in their non-production environment
- emacs integration for ocamlfdo build commands (fe?)
- benchmark the impact of building the compiler with ocamlfdo (bootstrap problem? cross?)
- implement better function reordering heuristics
- implement better block reordering heuristics
- find a safe way to reuse perf profile upon code changes
- benchmark ocamlfdo vs ocamlopt build for time and memory consumption
- check if ppx or other executables are changed in build2
- efficient format for decoded profiles (service?)
- release ocamlfdo on opam to get more users and developers
- integrate with dune

